{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "model_name = \"platune_maestrov4_win2\"\n",
    "step = None\n",
    "device_id = \"2\"\n",
    "test = True\n",
    "suffix = \"\"\n",
    "data = \"real\"\n",
    "max_samples = 1000\n",
    "N_SIGNAL = 131072\n",
    "batch_size = 16\n",
    "NB_STEPS = 10\n",
    "SEED = 42\n",
    "DO_CLAP = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/data/nils/repos2/codecs_benchmark/evaluation\")\n",
    "os.chdir(\"/data/nils/repos2/platune\")\n",
    "\n",
    "import json\n",
    "import torch\n",
    "\n",
    "from IPython.display import display, Audio\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "from after.dataset import CombinedDataset, SimpleDataset\n",
    "from after.autoencoder.wrappers import M2LWrapper\n",
    "\n",
    "import gin\n",
    "\n",
    "gin.enter_interactive_mode()\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "device = \"cuda:2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/data/nils/datasets/maestro/m2l_midiv2\"\n",
    "dataset = SimpleDataset(path, keys=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platune.model import PLaTune, SDEdit\n",
    "\n",
    "gin.clear_config()\n",
    "\n",
    "folder = os.path.join(\"/data/nils/repos2/platune/runs\", model_name)\n",
    "autoencoder_path = \"music2latent\"\n",
    "config = os.path.join(folder, \"config.gin\")\n",
    "\n",
    "# parse config\n",
    "gin.parse_config_file(config)\n",
    "\n",
    "paths = []\n",
    "\n",
    "for sub in [\"version_4\", \"version_3\", \"version_2\", \"version_1\", \"version_0\"]:\n",
    "    for subsub in [\n",
    "            \"last-v1.ckpt\",\n",
    "            \"last.ckpt\",\n",
    "    ]:\n",
    "        paths.append(os.path.join(folder, sub, \"checkpoints\", subsub))\n",
    "\n",
    "for p in paths:\n",
    "    try:\n",
    "        print(p)\n",
    "        state_dict = torch.load(p, map_location=\"cpu\")[\"state_dict\"]\n",
    "        break\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "if \"sdedit\" in model_name.lower():\n",
    "    model = SDEdit()\n",
    "    SDEDIT = True\n",
    "else:\n",
    "    model = PLaTune()\n",
    "    SDEDIT = False\n",
    "\n",
    "model.load_state_dict(state_dict, strict=True)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# Parse config\n",
    "gin.parse_config_file(config)\n",
    "SR = 44100\n",
    "try:\n",
    "    N_SIGNAL_LATENT = gin.query_parameter(\"%SEQ_LEN\")\n",
    "except:\n",
    "    N_SIGNAL_LATENT = 64\n",
    "print(N_SIGNAL)\n",
    "\n",
    "if autoencoder_path == \"music2latent\":\n",
    "    emb_model = M2LWrapper(device=device)\n",
    "    AE_RATIO = 4096\n",
    "else:\n",
    "    emb_model = torch.jit.load(autoencoder_path).eval().to(device)\n",
    "\n",
    "CONTINUOUS_KEYS = gin.query_parameter(\"%CONTINUOUS_KEYS\")\n",
    "DISCRETE_KEYS = gin.query_parameter(\"%DISCRETE_KEYS\")\n",
    "\n",
    "descriptors = DISCRETE_KEYS + CONTINUOUS_KEYS\n",
    "descriptors_nonmidi = [d for d in descriptors if d not in [\"pitch\", \"octave\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SIGNAL = AE_RATIO * N_SIGNAL_LATENT\n",
    "hop_length = AE_RATIO\n",
    "\n",
    "from platune.datasets.midi_descriptors import compute_midi_descriptors\n",
    "\n",
    "\n",
    "def get_midi(midi_data, chunk_number):\n",
    "    length = N_SIGNAL / SR\n",
    "    tstart = chunk_number * N_SIGNAL / SR\n",
    "    tend = (chunk_number + 1) * N_SIGNAL / SR\n",
    "    out_notes = []\n",
    "    for note in midi_data.instruments[0].notes:\n",
    "        if note.end > tstart and note.start < tend:\n",
    "            note.start = max(0, note.start - tstart)\n",
    "            note.end = min(note.end - tstart, length)\n",
    "            out_notes.append(note)\n",
    "\n",
    "    if len(out_notes) == 0:\n",
    "        return True, None\n",
    "    midi_data.instruments[0].notes = out_notes\n",
    "    # midi_data.adjust_times([0, length], [0, length])\n",
    "    return False, midi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx1 = 1079\n",
    "idx2 = 490\n",
    "\n",
    "\n",
    "def get_data(idx):\n",
    "    data = dataset[idx]\n",
    "\n",
    "    z = data[\"z\"][..., :N_SIGNAL_LATENT]\n",
    "\n",
    "    silence_test, midi_cur = get_midi(data[\"midi\"], 0)\n",
    "\n",
    "    out = compute_midi_descriptors(midi_cur,\n",
    "                                   target_length=N_SIGNAL_LATENT,\n",
    "                                   total_time=N_SIGNAL / SR)\n",
    "\n",
    "    z = torch.from_numpy(z).unsqueeze(0)\n",
    "\n",
    "    print([a.shape for a in out.values()])\n",
    "\n",
    "    cont_features = torch.stack(\n",
    "        [torch.from_numpy(out[desc]).float() for desc in descriptors])\n",
    "\n",
    "    print(cont_features.shape)\n",
    "    return z, out, cont_features\n",
    "\n",
    "\n",
    "z1, descriptors1, cont_features1 = get_data(idx1)\n",
    "\n",
    "z2, descriptors2, cont_features2 = get_data(idx2)\n",
    "\n",
    "audio1 = emb_model.decode(z1.to(device)).squeeze(0).cpu().numpy()\n",
    "display(Audio(audio1, rate=SR))\n",
    "\n",
    "audio2 = emb_model.decode(z2.to(device)).squeeze(0).cpu().numpy()\n",
    "display(Audio(audio2, rate=SR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_STEPS = 50\n",
    "\n",
    "a = model.process_attributes(torch.tensor([]), cont_features1, label=None)\n",
    "c = model.normalize_attr(a)\n",
    "\n",
    "cs_rec = model.z_to_cs(z1, c=c, nb_steps=NB_STEPS)\n",
    "cs_rec2 = model.z_to_cs(z2, c=c, nb_steps=NB_STEPS)\n",
    "\n",
    "c_rec = cs_rec[:, :model.control_dim]\n",
    "s_rec = cs_rec[:, model.control_dim:]\n",
    "\n",
    "c_dist, s_dist = model.get_cs_distributions(c, warmup=False, zero_var=False)\n",
    "cs = model.get_cs_samples(c_dist, s_dist)\n",
    "\n",
    "cs[:, :model.control_dim] = c_rec\n",
    "# z_sample = model.cs_to_z(cs, nb_steps=NB_STEPS)\n",
    "\n",
    "cs = cs_rec.clone()\n",
    "cs[:, :model.control_dim] = cs_rec2[:, :model.control_dim]\n",
    "z_sample_swap = model.cs_to_z(cs, nb_steps=NB_STEPS)\n",
    "\n",
    "# z_rec = model.cs_to_z(cs_rec, c=c, nb_steps=NB_STEPS)\n",
    "\n",
    "# Variation 1\n",
    "cs_var = cs_rec.clone()\n",
    "cs_var[:, 3] += 1.\n",
    "c_var = cs_var[:, :model.control_dim]\n",
    "\n",
    "# Variation 1\n",
    "# cs_var = cs_rec\n",
    "# c_var = c.clone()\n",
    "# c_var[:, 0] += 0.5\n",
    "\n",
    "z_var = model.cs_to_z(cs_var, c=c_var, nb_steps=NB_STEPS)\n",
    "\n",
    "display(Audio(audio1, rate=SR))\n",
    "\n",
    "# audio_sample = emb_model.decode(z_sample.to(device)).squeeze(0).cpu().numpy()\n",
    "# display(Audio(audio_sample, rate=SR))\n",
    "\n",
    "audio_sample = emb_model.decode(\n",
    "    z_sample_swap.to(device)).squeeze(0).cpu().numpy()\n",
    "display(Audio(audio_sample, rate=SR))\n",
    "\n",
    "# audio_rec = emb_model.decode(z_rec.to(device)).squeeze(0).cpu().numpy()\n",
    "# display(Audio(audio_rec, rate=SR))\n",
    "\n",
    "audio_var = emb_model.decode(z_var.to(device)).squeeze(0).cpu().numpy()\n",
    "display(Audio(audio_var, rate=SR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for i, descr in enumerate(descriptors):\n",
    "    print(f\"{descr}\")\n",
    "\n",
    "    plt.figure(figsize=(12, 2))\n",
    "    plt.title(f\"{descr}\")\n",
    "    plt.plot(c[0, i].cpu().numpy(), label=\"Original Normalized\")\n",
    "    plt.plot(c_rec[0, i].cpu().numpy(), label=\"Extracted\")\n",
    "    plt.plot(c_var[0, i].cpu().numpy(), label=\"Modified\")\n",
    "    # plt.ylim(-4, 4)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
